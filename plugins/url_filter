#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys, re, urllib.request, urllib.parse, urllib.error, json, datetime

from html.parser import HTMLParser
from html.entities import name2codepoint

from gzip import GzipFile
from io import BytesIO

import postgresql.driver.dbapi20 as postgresql
conn = postgresql.connect(database=sys.argv[1], user='termibot', password='PASSWORD', host='127.0.0.1')
cur = conn.cursor()


class MyUrlOpener(urllib.request.FancyURLopener):
    version = "Mozilla/4.0"
    def prompt_user_passwd(self, host, realm):
        pass
urllib.request._urlopener=MyUrlOpener()


class TitleParser(HTMLParser):
    title = ""
    _in_title = False

    def handle_starttag(self, tag, attrs):
        if tag == 'title':
            self._in_title = True
    def handle_endtag(self, tag):
        if tag == 'title':
            self._in_title = False
    def handle_data(self, data):
        if self._in_title:
            self.title += data

    def handle_entityref(self, name):
        if self._in_title:
            c = chr(name2codepoint[name])
            self.title += c

    def handle_charref(self, name):
        if self._in_title:
            if name.startswith('x'):
                c = chr(int(name[1:], 16))
            else:
                c = chr(int(name))
            self.title += c


def parse(line):
    uri_prefix = r'((http://|https://|ftp://)[\w\-\@;\/?:&=%\$_.+!*\x27(),~#]+[\w\-\@;\/?&=%\$_+!*\x27()~])'
    uri_re = re.compile(uri_prefix)
    return uri_re.findall(line)

def shorten(url):
    req = urllib.request.Request('https://www.googleapis.com/urlshortener/v1/url',
            json.dumps({'longUrl': url}).encode('utf-8'),
            {'Content-Type': 'application/json'})
    try:
        data = urllib.request.urlopen(req)
    except:
        return ''

    meta = json.loads(data.read().decode('utf-8'))
    if 'id' in meta:
        return meta['id']
    return ''

def gettitle(url):
    try:
        data = urllib.request.urlopen(url)
    except:
        return "unknown"

    ctype = data.info()["Content-Type"]
    if ";" in ctype:
        (ctype, charset) = data.info()["Content-Type"].split(";")[0:2]
        charset = charset.split("=")[1].lower()
    else:
        charset = 'utf-8'

    if ctype in ["text/html", "application/xhtml+xml"]:
        parser = TitleParser()

        if data.headers.get("content-encoding") == "gzip":
            data = gzip.GzipFile(fileobj=BytesIO(data.read()), mode="r")

        try:
            text = data.read().decode(charset)
        except:
            return 'unknown'

        sane_text = re.sub(r'(<!\[CDATA\[.*?\]\]>)', '', text, flags=re.DOTALL)
        try:
            parser.feed(sane_text)
        except:
            titles = list(re.findall('<title>(.*?)<', text))
            if len(titles) > 0:
                return titles[0]

        title = parser.title
        if title != None and title != '':
            return re.sub(r"[\n\s]+"," ", title.strip())
        else:
            return "(%s)" % ctype
    else:
        return "(%s)" % ctype

def lookup_twitter(url):
    parts = [i for i in url.split('/') if i != '']
    tweet = 'http://api.twitter.com/1/statuses/show/%s.json' % parts[-1]
    try:
        req = urllib.request.urlopen(tweet)
    except:
        return '[ unknown ]'

    data = json.loads(req.read().decode('utf-8'))
    if 'retweeted_status' in data and data['retweeted_status']:
        data = data['retweeted_status']
    data['text'] = " ".join(data['text'].split('\n'))
    return '%s (%s): %s (%s)' % (data['user']['name'], data['user']['screen_name'], data['text'], data['created_at'])

def lookup(url, nick):
    if re.match(r'https?://([^.]*\.)?twitter.com/.*status(es)?/[0-9]+', url):
        return lookup_twitter(url)

    # if url in database return database[url].{short,timestamp}
    cur.execute('SELECT * FROM urls WHERE LOWER(url) = LOWER(%s) OR LOWER(short_url) = LOWER(%s)', (url,url))
    res = cur.fetchone()
    if res:
        if len(res[2]) < 1:
            short = shorten(url)
            cur.execute('UPDATE urls SET short_url = %s WHERE url = %s', (short, res[0]))
            conn.commit()
        else:
            short = res[2]
        timestamp = res[3].strftime("%b %d, %Y at %H:%M")
        return "[ %s — %s ] (first posted by %s at %s)" % (short, gettitle(url), res[3][0]+"\x0f"+res[3][1:], timestamp)
    else:
        # else shorten, store with timestamp
        short = shorten(url)
        cur.execute('INSERT INTO urls VALUES(%s, NOW(), %s, %s)', (url, short, nick))
        conn.commit()
        return "[ %s — %s ]" % (short, gettitle(url))

if __name__ == '__main__':
    nick, user, host, sender, channel = sys.stdin.readline().split()[0:5]
    line = sys.stdin.readline().strip()
    urls = [url[0] for url in parse(line)]
    for url in urls:
        res = lookup(url, nick)
        if res:
            print(channel + " " + res)
