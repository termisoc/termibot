#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys, re, urllib.request, urllib.parse, urllib.error, json

from html.parser import HTMLParser

from gzip import GzipFile
from io import BytesIO

import postgresql.driver.dbapi20 as postgresql
conn = postgresql.connect(database=sys.argv[1], user='termibot', password='PASSWORD', host='127.0.0.1')
cur = conn.cursor()


class MyUrlOpener(urllib.request.FancyURLopener):
    version = "Mozilla/4.0"
    def prompt_user_passwd(self, host, realm):
        pass
urllib.request._urlopener=MyUrlOpener()


class TitleParser(HTMLParser):
    title = ""
    _in_title = False

    def handle_starttag(self, tag, attrs):
        if tag == 'title':
            self._in_title = True
    def handle_endtag(self, tag):
        if tag == 'title':
            self._in_title = False
    def handle_data(self, data):
        if self._in_title:
            self.title += data


def parse(line):
    uri_prefix = r'((http://|https://|ftp://)[\w\-\@;\/?:&=%\$_.+!*\x27(),~#]+[\w\-\@;\/?&=%\$_+!*\x27()~])'
    uri_re = re.compile(uri_prefix)
    return uri_re.findall(line)

def shorten(url):
    req = urllib.request.Request('https://www.googleapis.com/urlshortener/v1/url',
            json.dumps({'longUrl': url}).encode('utf-8'),
            {'Content-Type': 'application/json'})
    try:
        data = urllib.request.urlopen(req)
    except:
        return ''

    meta = json.loads(data.read().decode('utf-8'))
    if 'id' in meta:
        return meta['id']
    return ''

def gettitle(url):
    try:
        data = urllib.request.urlopen(url)
    except:
        return "unknown"

    ctype = data.info()["Content-Type"]
    if ";" in ctype:
        (ctype, charset) = data.info()["Content-Type"].split(";")[0:2]
        charset = charset.split("=")[1].lower()
    else:
        charset = 'utf-8'

    if ctype in ["text/html", "application/xhtml+xml"]:
        parser = TitleParser()

        if data.headers.get("content-encoding") == "gzip":
            data = gzip.GzipFile(fileobj=BytesIO(data.read()), mode="r")

        try:
            text = data.read().decode(charset)
        except:
            return 'unknown'

        sane_text = re.sub(r'(<!\[CDATA\[.*?\]\]>)', '', text, flags=re.DOTALL)
        try:
            parser.feed(sane_text)
        except:
            titles = list(re.findall('<title>(.*?)<', text))
            if len(titles) > 0:
                return titles[0]

        title = parser.title
        if title != None and title != '':
            return re.sub(r"[\n\s]+"," ", title.strip())
        else:
            return "(%s)" % ctype
    else:
        return "(%s)" % ctype

def lookup(url, nick):
    # if url in database return database[url].{short,timestamp}
    cur.execute('SELECT * FROM urls WHERE LOWER(url) = LOWER(%s)', (url,))
    res = cur.fetchone()
    if res:
        timestamp = res[1].strftime("%b %d, %Y at %H:%M")
        return "[ %s — %s ] (first posted by %s at %s)" % (res[2], gettitle(url), res[3][0]+"\x0f"+res[3][1:], timestamp)
    else:
        # else shorten, store with timestamp
        short = shorten(url)
        cur.execute('INSERT INTO urls VALUES(%s, NOW(), %s, %s)', (url, short, nick))
        conn.commit()
        return "[ %s — %s ]" % (short, gettitle(url))

if __name__ == '__main__':
    nick, user, host, sender, channel = sys.stdin.readline().split()[0:5]
    line = sys.stdin.readline().strip()
    urls = [url[0] for url in parse(line)]
    for url in urls:
        print(channel + " " + lookup(url, nick))
